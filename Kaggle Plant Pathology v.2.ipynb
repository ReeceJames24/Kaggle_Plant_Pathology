{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "special-evanescence",
   "metadata": {},
   "source": [
    "This notebook is running on C Desktop although the data folder is placed in D. \n",
    "\n",
    "Make use of this root path when needed:\n",
    "\n",
    "__D:\\Kaggle Plant Path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adjustable-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from shutil import copyfile\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-breakfast",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- ImageDataGenerator requires our images to be placed in subdirectories named after their respective labels. In our case, our images currently are unlabelled and are not divided into subdirectories. We are, however, given a csv file with the labels per image file.\n",
    "- We must first create subdirectories followed by a loop that places each training image in its corrrect subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rolled-symphony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800113bb65efe69e.jpg</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8002cb321f8bfcdf.jpg</td>\n",
       "      <td>scab frog_eye_leaf_spot complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80070f7fb5e2ccaa.jpg</td>\n",
       "      <td>scab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80077517781fb94f.jpg</td>\n",
       "      <td>scab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>800cbf0ff87721f8.jpg</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  image                           labels\n",
       "0  800113bb65efe69e.jpg                          healthy\n",
       "1  8002cb321f8bfcdf.jpg  scab frog_eye_leaf_spot complex\n",
       "2  80070f7fb5e2ccaa.jpg                             scab\n",
       "3  80077517781fb94f.jpg                             scab\n",
       "4  800cbf0ff87721f8.jpg                          complex"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get the labels by first importing the label csv\n",
    "root = 'D:\\Kaggle Plant Path'\n",
    "label_csv = os.path.join(root,'train.csv')\n",
    "df = pd.read_csv(label_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "brazilian-tsunami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complex',\n",
       " 'frog_eye_leaf_spot',\n",
       " 'frog_eye_leaf_spot complex',\n",
       " 'healthy',\n",
       " 'powdery_mildew',\n",
       " 'powdery_mildew complex',\n",
       " 'rust',\n",
       " 'rust complex',\n",
       " 'rust frog_eye_leaf_spot',\n",
       " 'scab',\n",
       " 'scab frog_eye_leaf_spot',\n",
       " 'scab frog_eye_leaf_spot complex']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(df['labels'])\n",
    "labels = list(np.unique(a))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lasting-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create primary directory and necessary subdirectories\n",
    "train_dir = os.path.join(root,'plants-training')\n",
    "to_create = [train_dir]\n",
    "for label in labels: #creates list of all necessary subdirectories to make\n",
    "    to_create.append(os.path.join(train_dir,label))\n",
    "\n",
    "#loop to create directories using paths in the to_create list\n",
    "'''for directory in to_create:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print(directory, 'created')\n",
    "    except:\n",
    "        print(directory, 'creation failed')''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "enhanced-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create alias variables for subdirectories for ease of reference\n",
    "'''var_list = ['complx', 'frog_eye_leaf_spot', 'frog_eye_leaf_spot_complx', 'healthy', 'powdery_mildew', 'powdery_mildew_complx','rust', 'rust_complx', 'rust_frog_eye_leaf_spot', 'scab', 'scab_frog_eye_leaf_spot', 'scab_frog_eye_leaf_spot_complx']\n",
    "class_dir = to_create[1:] #list that includes only subdirectory paths of the labels\n",
    "''';\n",
    "#this allows me to avoid the /f error when manually assigning the string of the path to variables (ex: \\\\frog_leaf would turn to \\x0rog_leaf)\n",
    "'''for (variable,subdir_path) in zip(var_list,class_dir):\n",
    "    exec(\"%s = %a\" % (variable, subdir_path))''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alternative-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over zip object of the file names and labels in train.csv and copy files to their respective subdirectories\n",
    "files = list(df['image'])\n",
    "y_train = list(df['labels'])\n",
    "\n",
    "images_dir = os.path.join(root,'train_images')\n",
    "\n",
    "'''\n",
    "for (file_name,label) in zip(files,y_train):\n",
    "    try:\n",
    "        if label == 'complex':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(complx,file_name))\n",
    "        elif label == 'frog_eye_leaf_spot':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(frog_eye_leaf_spot,file_name))\n",
    "        elif label == 'frog_eye_leaf_spot complex':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(frog_eye_leaf_spot_complx,file_name))\n",
    "        elif label == 'healthy':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(healthy,file_name))\n",
    "        elif label ==  'powdery_mildew':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(powdery_mildew,file_name))\n",
    "        elif label == 'powdery_mildew complex':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(powdery_mildew_complx,file_name))\n",
    "        elif label == 'rust':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path,os.path.join(rust,file_name))\n",
    "        elif label == 'rust complex':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(rust_complx,file_name))\n",
    "        elif label == 'rust frog_eye_leaf_spot':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path,os.path.join(rust_frog_eye_leaf_spot,file_name))\n",
    "        elif label == 'scab':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(scab,file_name))\n",
    "        elif label == 'scab frog_eye_leaf_spot':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(scab_frog_eye_leaf_spot,file_name))\n",
    "        elif label == 'scab frog_eye_leaf_spot complex':\n",
    "            file_path = os.path.join(images_dir,file_name)\n",
    "            copy2(file_path, os.path.join(scab_frog_eye_leaf_spot_complx,file_name))\n",
    "        else:\n",
    "            print('yo what disease is this')\n",
    "    except:\n",
    "        print('failed to copy ', file_name, ' with label ', label)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cardiovascular-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13977 images belonging to 12 classes.\n",
      "Found 4655 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "#Create ImageDataGenerators and apply image processing arguments (should find 12 classes)\n",
    "datagen = ImageDataGenerator(rescale = 1.0/255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode = 'nearest',\n",
    "                                   validation_split=0.25)\n",
    "                                   \n",
    "train_generator = datagen.flow_from_directory(train_dir,\n",
    "                                                    subset='training',\n",
    "                                                    target_size = (200,200),\n",
    "                                                    batch_size = 32,\n",
    "                                                    class_mode = 'sparse')\n",
    "validation_generator = datagen.flow_from_directory(train_dir,\n",
    "                                                    subset='validation',\n",
    "                                                    target_size = (200,200),\n",
    "                                                    batch_size = 32,\n",
    "                                                    class_mode = 'sparse')\n",
    "                                                   \n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "organizational-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callback \n",
    "\n",
    "class myCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc') is not None and logs.get('acc')>0.75):\n",
    "            print('Reached 75% training acc. Stopping Training!')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "disabled-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and compile Model\n",
    "'''callbacks = myCallback()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, (2,2), activation='relu', input_shape=(200,200,3)),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Conv2D(32, (2,2), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Conv2D(32,(2,2), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024,activation='relu'),\n",
    "    keras.layers.Dropout(0.20),\n",
    "    keras.layers.Dense(12,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "occupied-latin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train Model\n",
    "'''\n",
    "history = model.fit(train_generator, \n",
    "                    epochs = 10,\n",
    "                    validation_data = validation_generator,\n",
    "                    callbacks = [callbacks])\n",
    "\n",
    "#Create folder to save model in\n",
    "model_folder = os.path.join(root,\"model_v2\")\n",
    "os.mkdir(model_folder)\n",
    "model.save(model_folder)\n",
    "''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unexpected-chocolate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_files = train_generator.filenames\\nlen(train_files)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BONUS (for future ref): this is how you get the filenames taken by the generator\n",
    "'''\n",
    "train_files = train_generator.filenames\n",
    "len(train_files)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hydraulic-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: D:\\Kaggle Plant Path\\model_v2\\saved_model.pb/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c9e932a440cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'model_v2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'saved_model.pb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     81\u001b[0m                   (export_dir,\n\u001b[0;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: D:\\Kaggle Plant Path\\model_v2\\saved_model.pb/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "#Predict test data\n",
    "\n",
    "test_dir = os.path.join(root,'test_images')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
    "\n",
    "target_prediction_steps = 1\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size = (200,200),\n",
    "                                                    batch_size = target_prediction_steps,\n",
    "                                                    class_mode = None)\n",
    "\n",
    "test_images = test_generator.filenames\n",
    "num_samples = len(test_images)\n",
    "\n",
    "model_path = os.path.join(root,'model_v2')\n",
    "model = keras.models.load_model(os.path.join(model_path,'saved_model.pb'))\n",
    "\n",
    "predictions = model.predict(test_generator, steps = num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(train_generator.class_indices.keys()) # gives us the list of classes in the correct order\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's return the respective labels instead of the probability values\n",
    "max_predictions = predictions.argmax(axis=1) #shape of (3,)\n",
    "\n",
    "label_predictions = [classes[predictions] for predictions in max_predictions]\n",
    "label_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place image names and predicted labels in pd dataframe to convert to csv\n",
    "'''\n",
    "test_images = os.listdir(os.path.join(test_dir,'testing'))\n",
    "pred_arr = np.c_[test_images,label_predictions]\n",
    "type(pred_arr)\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert array to datafram \n",
    "'''\n",
    "submission_df = pd.DataFrame(data = pred_arr,\n",
    "                            index = None,\n",
    "                             columns = ['image', 'labels'])\n",
    "submission_df    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_df.to_csv('Chummi_Apple_Leaves_v2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-township",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-dominant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
